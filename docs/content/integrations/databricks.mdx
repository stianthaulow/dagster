---
title: "Databricks & Dagster | Dagster Docs"
description: Dagster can orchestrate Databricks jobs alongside other technologies.
---

# Databricks & Dagster

Dagster can orchestrate your Databricks jobs and other Databricks API calls, making it easy to chain together multiple Databricks jobs and orchestrate Databricks alongside your other technologies.

---

## Prerequisites

To get started, you will need to install the `dagster` and `dagster-databricks` Python packages:

```bash
pip install dagster dagster-databricks
```

You'll also want to have a Databricks workspace with an existing project that is deployed with a Databricks job. If you don't have this, [follow the Databricks quickstart](https://docs.databricks.com/workflows/jobs/jobs-quickstart.html) to set one up.

To manage your Databricks job from Dagster, you'll need three values, which can be set as [environment variables in Dagster](/guides/dagster/using-environment-variables-and-secrets):

1. A `host` for connecting with your Databricks workspace, starting with `https://`, stored in an environment variable `DATABRICKS_HOST`,
2. A `token` corresponding to a personal access token for your Databricks workspace, stored in an environment variable `DATABRICKS_TOKEN`, and
3. A `DATABRICKS_JOB_ID` for the Databricks job you want to run.

You can follow the [Databricks API authentication instructions](https://docs.databricks.com/dev-tools/python-api.html#step-1-set-up-authentication) to retrieve these values.

---

## Step 1: Connecting to Databricks

The first step in using Databricks with Dagster is to tell Dagster how to connect to your Databricks workspace using a Databricks [resource](/concepts/resources). This resource contains information on the location of your Databricks workspace and any credentials sourced from environment variables that are required to access it. You can access the underlying [Databricks API client](https://docs.databricks.com/dev-tools/python-api.html) to communicate to your Databricks workspace by configuring the resource.

For more information about the Databricks resource, see the [API reference](/\_apidocs/libraries/dagster-databricks).

```python startafter=start_define_databricks_client_instance endbefore=end_define_databricks_client_instance file=/integrations/databricks/databricks.py dedent=4
from dagster_databricks import databricks_client

databricks_client_instance = databricks_client.configured(
    {
        "host": {"env": "DATABRICKS_HOST"},
        "token": {"env": "DATABRICKS_TOKEN"},
    }
)
```

---

## Step 2: Create an op/asset that connects to Databricks

In this step, we'll demonstrate several ways to model a Databricks API call as either a Dagster [op](/concepts/ops-jobs-graphs/ops) or the computation backing an [asset definition](/concepts/assets/software-defined-assets). You can either:

- Use the `dagster-databricks` op factories, which create ops that invoke the Databricks Jobs' [Run Now](https://docs.databricks.com/api/workspace/jobs/runnow) ([`create_databricks_run_now_op`](/\_apidocs/libraries/dagster-databricks)) or [Submit Run](https://docs.databricks.com/api/workspace/jobs/submit) ([`create_databricks_submit_run_op`](/\_apidocs/libraries/dagster-databricks)) APIs, or
- Manually create a Dagster op or asset that connects to Databricks using the configured Databricks resource.

Afterward, we create a Dagster [job](/concepts/ops-jobs-graphs/jobs) that invokes the op or selects the asset to run the Databricks API call.

For guidance on deciding whether to use an op or asset, refer to the [Understanding how assets relate to ops guide](/guides/dagster/how-assets-relate-to-ops-and-graphs).

<TabGroup>

<TabItem name="Using op factories">

```python startafter=start_define_databricks_op_factories endbefore=end_define_databricks_op_factories file=/integrations/databricks/databricks.py dedent=4
from dagster_databricks import create_databricks_run_now_op

my_databricks_run_now_op = create_databricks_run_now_op(
    databricks_job_id=DATABRICKS_JOB_ID,
)

@job(resource_defs={"databricks": databricks_client_instance})
def my_databricks_job():
    my_databricks_run_now_op()
```

</TabItem>

<TabItem name="Creating a custom op using resources">

```python startafter=start_define_databricks_custom_op endbefore=end_define_databricks_custom_op file=/integrations/databricks/databricks.py dedent=4
from databricks_cli.sdk import DbfsService

from dagster import AssetExecutionContext, job, op

@op(required_resource_keys={"databricks"})
def my_databricks_op(context: AssetExecutionContext) -> None:
    databricks_api_client = context.resources.databricks.api_client
    dbfs_service = DbfsService(databricks_api_client)

    dbfs_service.read(path="/tmp/HelloWorld.txt")

@job(resource_defs={"databricks": databricks_client_instance})
def my_databricks_job():
    my_databricks_op()
```

</TabItem>

<TabItem name="Creating a custom asset using resources">

```python startafter=start_define_databricks_custom_asset endbefore=end_define_databricks_custom_asset file=/integrations/databricks/databricks.py dedent=4
from databricks_cli.sdk import JobsService

from dagster import AssetExecutionContext, AssetSelection, asset, define_asset_job

@asset(required_resource_keys={"databricks"})
def my_databricks_table(context: AssetExecutionContext) -> None:
    databricks_api_client = context.resources.databricks.api_client
    jobs_service = JobsService(databricks_api_client)

    jobs_service.run_now(job_id=DATABRICKS_JOB_ID)

materialize_databricks_table = define_asset_job(
    name="materialize_databricks_table",
    selection=AssetSelection.assets(my_databricks_table),
)
```

</TabItem>

</TabGroup>

---

## Step 3: Schedule your Databricks computation

Now that your Databricks API calls are modeled within Dagster, you can [schedule](/concepts/automation/schedules) them to run regularly.

In the example below, we schedule the `materialize_databricks_table` and `my_databricks_job` jobs to run daily:

```python startafter=start_schedule_databricks endbefore=end_schedule_databricks file=/integrations/databricks/databricks.py dedent=4
from dagster import AssetSelection, Definitions, ScheduleDefinition

defs = Definitions(
    assets=[my_databricks_table],
    schedules=[
        ScheduleDefinition(
            job=materialize_databricks_table,
            cron_schedule="@daily",
        ),
        ScheduleDefinition(
            job=my_databricks_job,
            cron_schedule="@daily",
        ),
    ],
    jobs=[my_databricks_job],
    resources={"databricks": databricks_client_instance},
)
```

---

## What's next?

By now, you should have a working Databricks and Dagster integration!

What's next? From here, you can:

- Learn more about [asset definitions](/concepts/assets/software-defined-assets)
- Check out the [`dagster-databricks` API docs](/\_apidocs/libraries/dagster-databricks)
